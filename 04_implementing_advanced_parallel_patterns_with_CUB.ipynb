{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Advanced Parallel Patterns with CUB\n",
    "In previous notebooks, we explored how to directly write CUDA kernels using CuPy's `RawModule` to implement basic parallel processing patterns like map and stencil.\n",
    "Specifically, in the last notebook, we delved into Shared Memory for cooperative processing within GPU blocks and Warp-level data exchange leveraging shuffle instructions.\n",
    "These concrete examples of Warp and block-level implementations laid crucial groundwork for understanding CUB (CUDA UnBound).\n",
    "\n",
    "This notebook introduces CUB, a high-performance library provided by NVIDIA, to tackle more challenging parallelization patterns such as reduction and scan.\n",
    "CUB offers a rich set of functionalities to implement frequently used GPU programming patterns with high efficiency and versatility.\n",
    "\n",
    "As a specific example for the reduction pattern, we'll focus on vector normalization.\n",
    "Through this process, we'll demonstrate how to use CUB's `BlockReduce` to efficiently compute the norm in parallel.\n",
    "Subsequently, we'll use prefix sum as our subject to explain the scan pattern, implementing it with CUB's `BlockScan`.\n",
    "\n",
    "By understanding these advanced parallel patterns and the importance of Coalesced Access to memory, you'll gain a foothold for optimizing more complex GPU computations and significantly boosting application performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "if os.environ.get('NVCC') is None:\n",
    "  os.environ['NVCC'] = '/usr/local/cuda/bin/nvcc'\n",
    "import cupy as cp\n",
    "\n",
    "err_eps = 1E-7\n",
    "block_size = 1024\n",
    "items_per_thread = 8\n",
    "length = block_size * items_per_thread\n",
    "\n",
    "dn = os.path.join(os.getcwd(), 'kernels')\n",
    "fpfn = os.path.join(dn, '04_advanced_parallel_patterns_1.cu')\n",
    "with open(fpfn, 'r') as f:\n",
    "  cuda_source = f.read()\n",
    "print(cuda_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `vectorNormalization` kernel calculates the vector normalization in parallel.\n",
    "\n",
    "First, it uses `BlockLoad` to read data from global memory.\n",
    "The `cub::BLOCK_LOAD_STRIPED` pattern used here is a technique to maximize GPU memory efficiency.\n",
    "GPUs can achieve higher speeds by reading and writing multiple threads to contiguous memory addresses simultaneously, a process known as Coalesced Access.\n",
    "In `cub::BLOCK_LOAD_STRIPED`, each thread within a thread block reads elements from global memory at regular intervals, such as `32` elements (Warp size).\n",
    "For example, thread `0` reads elements `0`, `32`, `64`, and so on, while thread `1` reads elements `1`, `33`, `65`, and so on.\n",
    "The `threadData` array held by each thread stores these interleaved elements sequentially.\n",
    "For computations where the order of individual elements doesn't affect the result, like the sum of squares calculation in this kernel, the performance benefits of Coalesced Access using the `STRIPED` pattern are significant, making it a suitable choice.\n",
    "For more details on data load patterns, refer to [the Flexible Data Arrangement section of the CUB official documentation](https://nvidia.github.io/cccl/cub/index.html#flexible-data-arrangement).\n",
    "Similarly, when writing computed results back to global memory, `cub::BLOCK_STORE_STRIPED` ensures efficient Coalesced Access for writes.\n",
    "\n",
    "CUB primitives such as `BlockLoad`, `BlockReduce`, and the subsequent `BlockShuffle` and `BlockStore` internally utilize fast Shared Memory as a temporary workspace.\n",
    "In this kernel, to efficiently manage the temporary storage used by these primitives, we place them in the same physical memory space using `__shared__ union { ... } tempStorage;`.\n",
    "This is a practically important optimization for efficiently reusing the limited GPU Shared Memory and enhancing kernel execution efficiency.\n",
    "\n",
    "Next, we calculate the sum of squares of the vector.\n",
    "This is a crucial parallel pattern called reduction, which aggregates many elements into a single result.\n",
    "Each thread handles a number of elements specified by `ITEMS_PER_THREAD`, individually squaring them and storing them in `threadDataSqr`.\n",
    "These squared values are then efficiently aggregated by `cub::BlockReduce` to compute the sum of squares, `sqrSum`.\n",
    "Such an implementation is possible because the order of individual elements does not affect the final sum of squares.\n",
    "\n",
    "`__syncthreads();` is critical for inter-thread cooperative computations like reduction.\n",
    "This synchronization is essential to ensure that all threads within the block have completed squaring their assigned elements before `BlockReduce` begins its sum of squares calculation.\n",
    "Without data fully prepared, accurate results cannot be obtained.\n",
    "This synchronization also ensures that `BlockLoad` and `BlockReduce` can safely reuse the same Shared Memory region via the union.\n",
    "While CUB primitives perform necessary internal synchronization, explicit synchronization by the developer is crucial when sharing Shared Memory between different primitives and guaranteeing the order of operations and data readiness.\n",
    "\n",
    "Subsequently, we compute the norm using the calculated sum of squares and then normalize each element.\n",
    "The sum of squares, which is the result of `BlockReduce`, holds a valid value only for the thread with `threadIdx.x == 0` within the thread block (the leader thread).\n",
    "To normalize elements per thread, the norm value is needed by all threads in the block.\n",
    "Therefore, we use `cub::BlockShuffle` to broadcast this value.\n",
    "We perform another `__syncthreads();` here to ensure that all normalization calculations are complete.\n",
    "In broadcasting using `BlockShuffle`, we pass an array `normDummy[1]` as an argument.\n",
    "This is because the `cub::BlockShuffle::Down` method is designed to receive the source value for broadcasting as a reference to an array.\n",
    "\n",
    "Finally, we use `BlockStore` to write the normalized vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_source = cuda_source.replace('ADVANCED_PARALLEL_PATTERNS_1_BLOCK_SIZE', str(block_size))\n",
    "cuda_source = cuda_source.replace('ADVANCED_PARALLEL_PATTERNS_1_ITEMS_PER_THREAD', str(items_per_thread))\n",
    "module = cp.RawModule(code=cuda_source, backend='nvcc')\n",
    "module.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For compiling this kernel, we specify `backend='nvcc'`.\n",
    "This is necessary because we are using official NVIDIA CUDA libraries like CUB.\n",
    "As explained previously, CuPy's default compiler may not support these, so we explicitly use `nvcc`.\n",
    "\n",
    "Finally, we execute the GPU kernel using CuPy and verify the results by comparing them with a NumPy calculation.\n",
    "If the computation is successful, no assertion error will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand((length)).astype(np.float32)\n",
    "x_gpu = cp.array(x, dtype=cp.float32)\n",
    "y_gpu = cp.empty_like(x)\n",
    "assert x_gpu.flags.c_contiguous\n",
    "assert y_gpu.flags.c_contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_func = module.get_function('vectorNormalization')\n",
    "sz_block = block_size,\n",
    "sz_grid = 1,\n",
    "gpu_func(\n",
    "  block=sz_block, grid=sz_grid,\n",
    "  args=(y_gpu, x_gpu)\n",
    ")\n",
    "cp.cuda.runtime.deviceSynchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_gpu.get()\n",
    "y_ref = x / np.linalg.norm(x)\n",
    "\n",
    "err = np.abs(y_ref - y)\n",
    "assert np.max(err) < err_eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, we've explained three important elements of GPU parallel processing, using vector normalization as an example.\n",
    "Specifically, we've looked at the reduction pattern (sum of squares), broadcasting values using `BlockShuffle`, and the crucial `STRIPED` reads/writes and Coalesced Access for performance.\n",
    "\n",
    "Next, we'll explain a different parallel pattern: prefix sum (scan pattern). This scan has different characteristics and implementation considerations compared to reduction.\n",
    "\n",
    "First, let's read the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn = os.path.join(os.getcwd(), 'kernels')\n",
    "fpfn = os.path.join(dn, '04_advanced_parallel_patterns_2.cu')\n",
    "with open(fpfn, 'r') as f:\n",
    "  cuda_source = f.read()\n",
    "print(cuda_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at `parallelPrefixSum`, which implements prefix sum (cumulative sum).\n",
    "This operation calculates the total value up to each element in a given sequence.\n",
    "For example, the prefix sum of `[1, 2, 3, 4]` is `[1, 3, 6, 10]`.\n",
    "This is called the scan pattern in the context of parallel algorithms and is generally difficult to parallelize because each output element depends on all preceding input elements.\n",
    "\n",
    "This kernel also uses `BlockLoad` and `BlockStore` to read and write data.\n",
    "However, unlike the previous normalization kernel, it's crucial to note that here we specify the `cub::BLOCK_LOAD_DIRECT` and `cub::BLOCK_STORE_DIRECT` patterns.\n",
    "Since the order of elements directly affects the result in prefix sum, the data arrangement held by each thread must be contiguous in global memory.\n",
    "In the `DIRECT` pattern, the `ITEMS_PER_THREAD` elements handled by each thread read and write a contiguous range in global memory (e.g., thread `0` reads from element `0` to `ITEMS_PER_THREAD-1`, thread `1` reads from `ITEMS_PER_THREAD` to `2*ITEMS_PER_THREAD-1`, and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll compile and execute this kernel, then verify its results.\n",
    "This time, we've changed the element type to `int32`.\n",
    "For verification, we'll use NumPy's `numpy.cumsum()` to calculate the reference cumulative sum and compare it with the GPU result.\n",
    "If the computation is successful, no assertion will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_source = cuda_source.replace('ADVANCED_PARALLEL_PATTERNS_2_BLOCK_SIZE', str(block_size))\n",
    "cuda_source = cuda_source.replace('ADVANCED_PARALLEL_PATTERNS_2_ITEMS_PER_THREAD', str(items_per_thread))\n",
    "module = cp.RawModule(code=cuda_source, backend='nvcc')\n",
    "module.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (1024 * np.random.rand((length))).astype(np.int32)\n",
    "x_gpu = cp.array(x, dtype=cp.int32)\n",
    "y_gpu = cp.empty_like(x)\n",
    "assert x_gpu.flags.c_contiguous\n",
    "assert y_gpu.flags.c_contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_func = module.get_function('parallelPrefixSum')\n",
    "sz_block = block_size,\n",
    "sz_grid = 1,\n",
    "gpu_func(\n",
    "  block=sz_block, grid=sz_grid,\n",
    "  args=(y_gpu, x_gpu)\n",
    ")\n",
    "cp.cuda.runtime.deviceSynchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_gpu.get()\n",
    "y_ref = np.cumsum(x)\n",
    "\n",
    "err = np.abs(y_ref - y)\n",
    "assert np.max(err) < err_eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we covered two advanced methods for implementing parallel patterns in CUDA: vector normalization as an example of reduction, and prefix sum (cumulative sum) as an example of scan.\n",
    "For both patterns, we demonstrated through fundamental and concrete code how powerful CUB is in maximizing GPU performance.\n",
    "\n",
    "First, for vector normalization, we explained the flow from independent element calculation and reduction (sum of squares) to broadcasting the result.\n",
    "Notably, we implemented Coalesced Access, crucial for achieving GPU memory efficiency, using patterns like `cub::BLOCK_LOAD_STRIPED` and `cub::BLOCK_STORE_STRIPED`.\n",
    "\n",
    "Next, for prefix sum, we saw how `cub::BlockScan` can concisely describe the complex logic inherent in the scan pattern, which has a particular difficulty in parallelization due to each element depending on its predecessors.\n",
    "Here, we also showed that a different load pattern, `cub::BLOCK_LOAD_DIRECT`, is more suitable because data order is critical.\n",
    "\n",
    "Common to both kernels was the importance of using a union struct to efficiently reuse the limited Shared Memory resource, and the necessity of explicit `__syncthreads()` for guaranteeing computational correctness.\n",
    "While CUB encapsulates complex low-level synchronization, developers must manage synchronization timing themselves when ensuring that data for CUB operations is fully computed or when reusing Shared Memory between different CUB primitives.\n",
    "\n",
    "Thus, we've shown how complex parallel patterns can be implemented efficiently and concisely by leveraging CUB."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy-cuda-20250531",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
