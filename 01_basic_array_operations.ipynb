{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Array Operations\n",
    "This notebook explains how to execute CUDA kernels using CuPy's `RawModule`.\n",
    "\n",
    "The fundamental process involves reading CUDA kernel source code from a text file, compiling it, and then executing it as a function.\n",
    "This section also covers using string replacement for compile-time constant embedding (similar to a preprocessor), leveraging CUDA's constant memory, and measuring execution time with CuPy's Event objects.\n",
    "The CUDA kernels discussed here are well-suited for implementing the map pattern in parallel computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "dn = os.path.join(os.getcwd(), 'kernels')\n",
    "fpfn = os.path.join(dn, '01_basic_array_operations_1.cu')\n",
    "with open(fpfn, 'r') as f:\n",
    "  cuda_source = f.read()\n",
    "module = cp.RawModule(code=cuda_source)\n",
    "module.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`01_basic_array_operations_1.cu` implements constant scaling of arrays as an example of basic array operations.\n",
    "The CUDA kernel, read as a string, is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extern \"C\" __global__ void mult(float *x, float a, int length)\n",
      "{\n",
      "  const int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "  if (index >= length)\n",
      "  {\n",
      "    return;\n",
      "  }\n",
      "  x[index] *= a;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cuda_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CUDA, the smallest unit of processing is a thread. These threads are grouped into thread blocks. A single thread block can contain a maximum of 1024 threads.\n",
    "This limit exists because all threads within a block are expected to reside on the same streaming multiprocessor core and must share that core's limited memory resources.\n",
    "\n",
    "Grids are formed when multiple thread blocks are combined. Essentially, a grid is a collection of thread blocks, enabling highly scalable parallel computation.\n",
    "Thread blocks within a grid execute independently.\n",
    "\n",
    "Let's clarify some key identifiers and built-in variables:\n",
    "The `__global__` identifier is used in CUDA C++ to define a kernel.\n",
    "Unlike regular C++ functions, kernels are executed in parallel by multiple CUDA threads when called.\n",
    "`blockIdx` is a built-in variable that provides a unique index for each thread block within the grid.\n",
    "It can be accessed as a 1D, 2D, or 3D index, helping to identify which block the current thread belongs to within the kernel.\n",
    "`blockDim` is a built-in variable indicating the dimensions of a thread block.\n",
    "Specifically, it provides the number of threads (sizes in x, y, and z directions) contained within each thread block.\n",
    "This is useful for calculating a thread's global index.\n",
    "`threadIdx` is another built-in variable accessible within the kernel, providing a unique thread ID assigned to each thread executing the kernel.\n",
    "`threadIdx` is treated as a 3-component vector, allowing threads to be identified using 1D, 2D, or 3D thread indices.\n",
    "This helps threads within a block identify their specific roles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CuPy allows direct uploading of NumPy `numpy.array objects` to the GPU as `cupy.array` objects.\n",
    "For demonstration, a random array of length 65536 is generated and uploaded to the GPU.\n",
    "Additionally, a random scalar for constant multiplication and a threshold for verification are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 65536\n",
    "err_eps = 1E-6\n",
    "x = np.random.rand((length)).astype(np.float32)\n",
    "x_gpu = cp.array(x, dtype=cp.float32)\n",
    "a = np.random.rand(1,).astype(np.float32)\n",
    "a_gpu = cp.float32(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute the compiled CUDA kernel, the `mult` function is first retrieved as a function object using `get_function()`.\n",
    "The function object is executed by providing thread block and grid dimensions as a tuple, followed by the actual arguments for the CUDA kernel.\n",
    "Since CUDA kernels execute asynchronously on the GPU, explicit synchronization is performed to ensure correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_func = module.get_function('mult')\n",
    "sz_block = 1024,\n",
    "sz_grid = math.ceil(length / sz_block[0]),\n",
    "gpu_func(\n",
    "  block=sz_block, grid=sz_grid,\n",
    "  args=(x_gpu, a_gpu, length)\n",
    ")\n",
    "cp.cuda.runtime.deviceSynchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After execution, the results from the GPU are retrieved using the `get()` function and verified against NumPy's calculation.\n",
    "An assertion ensures the computation was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = x_gpu.get()\n",
    "err = np.abs((x2 / a) - x)\n",
    "assert np.max(err) < err_eps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compile-time constant embedding using string replacement in the source code is demonstrated.\n",
    "The CUDA kernel is read as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const float g_a(BASIC_ARRAY_OPTRATIONS_2_A);\n",
      "extern \"C\" __global__ void multConstant(float *x)\n",
      "{\n",
      "  const int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "  if (index >= BASIC_ARRAY_OPTRATIONS_2_LENGTH)\n",
      "  {\n",
      "    return;\n",
      "  }\n",
      "  x[index] *= g_a;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpfn = os.path.join(dn, '01_basic_array_operations_2.cu')\n",
    "with open(fpfn, 'r') as f:\n",
    "  cuda_source = f.read()\n",
    "print(cuda_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, as read, it cannot be compiled directly due to placeholder strings for constants (`BASIC_ARRAY_OPTRATIONS_2_A`) and array length (`BASIC_ARRAY_OPTRATIONS_2_LENGTH`).\n",
    "After reading, Python's standard library is used to perform string replacement.\n",
    "It's important to understand that values from Python variables are embedded as constants in the CUDA kernel at compile time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_source = cuda_source.replace('BASIC_ARRAY_OPTRATIONS_2_LENGTH', str(length))\n",
    "cuda_source = cuda_source.replace('BASIC_ARRAY_OPTRATIONS_2_A', str(float(a[0])))\n",
    "module = cp.RawModule(code=cuda_source)\n",
    "module.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPU function is then executed, with an assertion to verify calculation success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cp.array(x, dtype=cp.float32)\n",
    "\n",
    "gpu_func = module.get_function('multConstant')\n",
    "sz_block = 1024,\n",
    "sz_grid = math.ceil(length / sz_block[0]),\n",
    "gpu_func(\n",
    "  block=sz_block, grid=sz_grid,\n",
    "  args=(x_gpu)\n",
    ")\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "\n",
    "x2 = x_gpu.get()\n",
    "err = np.abs((x2 / a) - x)\n",
    "assert np.max(err) < err_eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement a CUDA kernel that utilizes CUDA's constant memory.\n",
    "\n",
    "This memory resides in the GPU's device memory and is cached by the constant cache.\n",
    "If the data is found in the cache, it's processed at the constant cache's throughput; otherwise, it's processed at the device memory's throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__constant__ float g_a; // constant memory\n",
      "extern \"C\" __global__ void multConstantMemory(float *x)\n",
      "{\n",
      "  const int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "  if (index >= BASIC_ARRAY_OPTRATIONS_3_LENGTH)\n",
      "  {\n",
      "    return;\n",
      "  }\n",
      "  x[index] *= g_a;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpfn = os.path.join(dn, '01_basic_array_operations_3.cu')\n",
    "with open(fpfn, 'r') as f:\n",
    "  cuda_source = f.read()\n",
    "print(cuda_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA's constant memory can be used by declaring variables with the `__constant__` memory space specifier.\n",
    "Here, the constant for multiplication is defined in constant memory, and the array length is defined as a regular constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_source = cuda_source.replace('BASIC_ARRAY_OPTRATIONS_3_LENGTH', str(length))\n",
    "module = cp.RawModule(code=cuda_source)\n",
    "module.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compilation, the constant memory pointer is obtained using the `get_global()` function, and the value is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ptr = module.get_global('g_a')\n",
    "a_gpu = cp.ndarray((1), dtype=cp.float32, memptr=ptr)\n",
    "a_gpu[:] = a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPU function is then executed, with an assertion to verify calculation success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cp.array(x, dtype=cp.float32)\n",
    "\n",
    "gpu_func = module.get_function('multConstantMemory')\n",
    "sz_block = 1024,\n",
    "sz_grid = math.ceil(length / sz_block[0]),\n",
    "gpu_func(\n",
    "  block=sz_block, grid=sz_grid,\n",
    "  args=(x_gpu)\n",
    ")\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "\n",
    "x2 = x_gpu.get()\n",
    "err = np.abs((x2 / a) - x)\n",
    "assert np.max(err) < err_eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extend the `multConstantMemory` kernel to demonstrate how to use multiple coefficients stored as a structure in constant memory.\n",
    "This kernel performs a fused multiply-add operation similar to the SAXPY ($y = ax + b$) operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#pragma pack(push, 4)\n",
      "struct multAddCoefficient\n",
      "{\n",
      "  float a, b;\n",
      "};\n",
      "#pragma pack(pop)\n",
      "\n",
      "__constant__ multAddCoefficient g_maCoef = {}; // constant memory\n",
      "\n",
      "extern \"C\" __global__ void getMACoefSize(int *sz)\n",
      "{\n",
      "  const int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "  if (index > 0)\n",
      "  {\n",
      "    return;\n",
      "  }\n",
      "  *sz = sizeof(multAddCoefficient);\n",
      "}\n",
      "\n",
      "extern \"C\" __global__ void multAddConstantMemory(float *x)\n",
      "{\n",
      "  const int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "  if (index >= BASIC_ARRAY_OPTRATIONS_4_LENGTH)\n",
      "  {\n",
      "    return;\n",
      "  }\n",
      "  x[index] = g_maCoef.a * x[index] + g_maCoef.b;\n",
      "  // use FMA for faster operation\n",
      "  // x[index] = fmaf(g_maCoef.a, x[index], g_maCoef.b);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpfn = os.path.join(dn, '01_basic_array_operations_4.cu')\n",
    "with open(fpfn, 'r') as f:\n",
    "  cuda_source = f.read()\n",
    "print(cuda_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this CUDA kernel, a POD (Plain Old Data) structure named `multAddCoefficient` is defined and declared as constant memory `g_maCoef`.\n",
    "The `#pragma pack(push, 4)` and `#pragma pack(pop)` directives are crucial here. They ensure the struct's members are tightly packed to a 4-byte alignment, accurately matching the Python-side data structure's memory layout for precise data transfer.\n",
    "\n",
    "Also, notice the commented-out line utilizing the `fmaf` (Fused Multiply-Add) function.\n",
    "This is included for optional experimentation, and its numerical implications will be discussed in more detail after the kernel execution.\n",
    "While `fmaf` generally offers performance and precision benefits by combining multiplication and addition into a single instruction, it's worth noting its potential for subtle differences in numerical outcomes compared to separate multiplication and addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_source = cuda_source.replace('BASIC_ARRAY_OPTRATIONS_4_LENGTH', str(length))\n",
    "module = cp.RawModule(code=cuda_source)\n",
    "module.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `multAddCoefficient` structure defined on the GPU side adheres to CUDA's data packing rules. Typically, `float` types are 4-byte aligned, so this structure, with `a` and `b` each being 4 bytes, totals 8 bytes and is 4-byte aligned.\n",
    "\n",
    "On the Python side, `numpy.dtype` is used to define a data type `dtype_ma_coef` that strictly matches the alignment and memory layout of the CUDA-side structure.\n",
    "`'a': (np.float32, 0)` means a 4-byte float starting at offset 0, and `'b': (np.float32, 4)` means a 4-byte float starting at offset 4, ensuring proper mapping between the CUDA structure and Python data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_ma_coef = np.dtype({'a': (np.float32, 0), 'b': (np.float32, 4)})\n",
    "\n",
    "sz_gpu = cp.empty(1, dtype=cp.int32)\n",
    "gpu_func_get_size = module.get_function('getMACoefSize')\n",
    "gpu_func_get_size(\n",
    "    block=(1,),\n",
    "    grid=(1,),\n",
    "    args=(sz_gpu,)\n",
    ")\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "sz = int(sz_gpu[0].get())\n",
    "assert sz == dtype_ma_coef.itemsize, \\\n",
    "    'Expected POD size {dtype_pod.itemsize}, but got {sz}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This structure is directly uploaded to the GPU's constant memory via a pointer obtained with `get_global()`, and then utilized within the kernel.\n",
    "Specifically, the `ma_coef` NumPy array, which is meticulously aligned to match the CUDA `multAddCoefficient` structure, is converted into a byte array using `tobytes()`.\n",
    "This byte array is then uploaded to the GPU's constant memory region that `ma_coef_gpu_constant` points to.\n",
    "This direct byte-level transfer ensures that the exact memory layout and values of the host-side POD structure are faithfully replicated in the device's constant memory, making them accessible to the CUDA kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.rand(1,).astype(np.float32)\n",
    "\n",
    "ma_coef = np.empty((1,), dtype=dtype_ma_coef)\n",
    "ma_coef[0]['a'] = a[0]\n",
    "ma_coef[0]['b'] = b[0]\n",
    "\n",
    "ma_coef_ptr = module.get_global('g_maCoef')\n",
    "ma_coef_gpu_constant = cp.ndarray((sz,), dtype=cp.byte, memptr=ma_coef_ptr)\n",
    "ma_coef_gpu_constant[:] = cp.frombuffer(ma_coef.tobytes(), dtype=np.byte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPU function is then executed, followed by an assertion to verify calculation success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cp.array(x, dtype=cp.float32)\n",
    "\n",
    "gpu_func = module.get_function('multAddConstantMemory')\n",
    "sz_block = 1024,\n",
    "sz_grid = math.ceil(length / sz_block[0]),\n",
    "gpu_func(\n",
    "  block=sz_block, grid=sz_grid,\n",
    "  args=(x_gpu)\n",
    ")\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "\n",
    "x2 = x_gpu.get()\n",
    "err = np.abs(((x2 - b)/ a) - x)\n",
    "assert np.max(err) < err_eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel executes the multiply-add operation: `x[index] = g_maCoef.a * x[index] + g_maCoef.b;`.\n",
    "\n",
    "This CUDA kernel also includes a commented-out line utilizing the `fmaf` (Fused Multiply-Add) function.\n",
    "It's important to note that using `fmaf` may result in slightly different numerical outcomes compared to performing separate multiplication and addition due to the nature of floating-point arithmetic.\n",
    "If the `fmaf` line is uncommented and the code is run with `err_eps = 1E-6`, the assertion might fail because the numerical result from `fmaf` could deviate slightly more than the threshold allows.\n",
    "This highlights how floating-point precision can vary depending on the exact sequence of operations.\n",
    "Those interested in exploring this can uncomment the line to compare performance and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this notebook explains how to measure execution time using CuPy's `Event` objects.\n",
    "Event objects allow for the recording of timestamps at specific points on the GPU.\n",
    "The `get_elapsed_time()` function is then used to calculate the elapsed time from these timestamps.\n",
    "\n",
    "This involves creating start and end event objects.\n",
    "`start.record()` marks the beginning of the timed section on the GPU.\n",
    "Immediately after, `start.synchronize()` ensures the CPU waits for this marker to be recorded, guaranteeing accurate timing.\n",
    "The GPU function (the target computation) is then called.\n",
    "Crucially, `cp.cuda.runtime.deviceSynchronize()` is invoked to ensure the GPU device finishes all its pending operations before `end.record()` is called.\n",
    "This is vital because GPU operations are asynchronous.\n",
    "Finally, `end.record()` marks the completion of the timed section, and `end.synchronize()` ensures the CPU waits for this final marker.\n",
    "The elapsed time in milliseconds is then retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 0.517952024936676 [msec]\n"
     ]
    }
   ],
   "source": [
    "start = cp.cuda.Event()\n",
    "end = cp.cuda.Event()\n",
    "\n",
    "start.record()\n",
    "start.synchronize()\n",
    "gpu_func(\n",
    "  block=sz_block, grid=sz_grid,\n",
    "  args=(x_gpu, a, length)\n",
    ")\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "end.record()\n",
    "end.synchronize()\n",
    "msec = cp.cuda.get_elapsed_time(start, end)\n",
    "print('Elapsed Time: {} [msec]'.format(msec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy-cuda-20250531",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
