{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Array Operations\n",
    "This notebook explains how to execute CUDA kernels using CuPy's `RawModule`.\n",
    "\n",
    "The fundamental process involves reading CUDA kernel source code from a text file, compiling it, and then executing it as a function.\n",
    "This section also covers using string replacement for compile-time constant embedding (similar to a preprocessor), leveraging CUDA's constant memory, and measuring execution time with CuPy's Event objects.\n",
    "The CUDA kernels discussed here are well-suited for implementing the map pattern in parallel computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "dn = os.getcwd()\n",
    "fpfn = os.path.join(dn, '01_basic_array_operations_1.cu')\n",
    "with open(fpfn, 'r') as f:\n",
    "  cuda_source = f.read()\n",
    "module = cp.RawModule(code=cuda_source)\n",
    "module.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`01_basic_array_operations_1.cu` implements constant scaling of arrays as an example of basic array operations.\n",
    "The CUDA kernel, read as a string, is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extern \"C\" __global__ void mult(float *x, float a, int length)\n",
      "{\n",
      "  const int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "  if (index >= length)\n",
      "  {\n",
      "    return;\n",
      "  }\n",
      "  x[index] *= a;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cuda_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CUDA, the smallest unit of processing is a thread. These threads are grouped into thread blocks. A single thread block can contain a maximum of 1024 threads.\n",
    "This limit exists because all threads within a block are expected to reside on the same streaming multiprocessor core and must share that core's limited memory resources.\n",
    "\n",
    "Grids are formed when multiple thread blocks are combined. Essentially, a grid is a collection of thread blocks, enabling highly scalable parallel computation.\n",
    "Thread blocks within a grid execute independently.\n",
    "\n",
    "Let's clarify some key identifiers and built-in variables:\n",
    "The `__global__` identifier is used in CUDA C++ to define a kernel.\n",
    "Unlike regular C++ functions, kernels are executed in parallel by multiple CUDA threads when called.\n",
    "`blockIdx` is a built-in variable that provides a unique index for each thread block within the grid.\n",
    "It can be accessed as a 1D, 2D, or 3D index, helping to identify which block the current thread belongs to within the kernel.\n",
    "`blockDim` is a built-in variable indicating the dimensions of a thread block.\n",
    "Specifically, it provides the number of threads (sizes in x, y, and z directions) contained within each thread block.\n",
    "This is useful for calculating a thread's global index.\n",
    "`threadIdx` is another built-in variable accessible within the kernel, providing a unique thread ID assigned to each thread executing the kernel.\n",
    "`threadIdx` is treated as a 3-component vector, allowing threads to be identified using 1D, 2D, or 3D thread indices.\n",
    "This helps threads within a block identify their specific roles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CuPy allows direct uploading of NumPy `numpy.array objects` to the GPU as `cupy.array` objects.\n",
    "For demonstration, a random array of length 65536 is generated and uploaded to the GPU.\n",
    "Additionally, a random scalar for constant multiplication and a threshold for verification are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 65536\n",
    "err_eps = 1E-7\n",
    "x = np.random.rand((length)).astype(np.float32)\n",
    "x_gpu = cp.array(x, dtype=cp.float32)\n",
    "a = np.random.rand(1,).astype(np.float32)\n",
    "a_gpu = cp.float32(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute the compiled CUDA kernel, the `mult` function is first retrieved as a function object using `get_function()`.\n",
    "The function object is executed by providing thread block and grid dimensions as a tuple, followed by the actual arguments for the CUDA kernel.\n",
    "Since CUDA kernels execute asynchronously on the GPU, explicit synchronization is performed to ensure correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_func = module.get_function('mult')\n",
    "sz_block = 1024,\n",
    "sz_grid = math.ceil(length / sz_block[0]),\n",
    "gpu_func(\n",
    "  block=sz_block, grid=sz_grid,\n",
    "  args=(x_gpu, a_gpu, length)\n",
    ")\n",
    "cp.cuda.runtime.deviceSynchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After execution, the results from the GPU are retrieved using the `get()` function and verified against NumPy's calculation.\n",
    "An assertion ensures the computation was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = x_gpu.get()\n",
    "err = np.abs((x2 / a) - x)\n",
    "assert np.max(err) < err_eps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compile-time constant embedding using string replacement in the source code is demonstrated.\n",
    "The CUDA kernel is read as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const float g_a(BASIC_ARRAY_OPTRATIONS_2_A);\n",
      "extern \"C\" __global__ void multConstant(float *x)\n",
      "{\n",
      "  const int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "  if (index >= BASIC_ARRAY_OPTRATIONS_2_LENGTH)\n",
      "  {\n",
      "    return;\n",
      "  }\n",
      "  x[index] *= g_a;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn = os.getcwd()\n",
    "fpfn = os.path.join(dn, '01_basic_array_operations_2.cu')\n",
    "with open(fpfn, 'r') as f:\n",
    "  cuda_source = f.read()\n",
    "print(cuda_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, as read, it cannot be compiled directly due to placeholder strings for constants (`BASIC_ARRAY_OPTRATIONS_2_A`) and array length (`BASIC_ARRAY_OPTRATIONS_2_LENGTH`).\n",
    "After reading, Python's standard library is used to perform string replacement.\n",
    "It's important to understand that values from Python variables are embedded as constants in the CUDA kernel at compile time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_source = cuda_source.replace('BASIC_ARRAY_OPTRATIONS_2_LENGTH', str(length))\n",
    "cuda_source = cuda_source.replace('BASIC_ARRAY_OPTRATIONS_2_A', str(float(a[0])))\n",
    "module = cp.RawModule(code=cuda_source)\n",
    "module.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPU function is then executed, with an assertion to verify calculation success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cp.array(x, dtype=cp.float32)\n",
    "\n",
    "gpu_func = module.get_function('multConstant')\n",
    "sz_block = 1024,\n",
    "sz_grid = math.ceil(length / sz_block[0]),\n",
    "gpu_func(\n",
    "  block=sz_block, grid=sz_grid,\n",
    "  args=(x_gpu)\n",
    ")\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "\n",
    "x2 = x_gpu.get()\n",
    "err = np.abs((x2 / a) - x)\n",
    "assert np.max(err) < err_eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement a CUDA kernel that utilizes CUDA's constant memory.\n",
    "\n",
    "This memory resides in the GPU's device memory and is cached by the constant cache.\n",
    "If the data is found in the cache, it's processed at the constant cache's throughput; otherwise, it's processed at the device memory's throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__constant__ float g_a; // constant memory\n",
      "extern \"C\" __global__ void multConstantMemory(float *x)\n",
      "{\n",
      "  const int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "  if (index >= BASIC_ARRAY_OPTRATIONS_3_LENGTH)\n",
      "  {\n",
      "    return;\n",
      "  }\n",
      "  x[index] *= g_a;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpfn = os.path.join(dn, '01_basic_array_operations_3.cu')\n",
    "with open(fpfn, 'r') as f:\n",
    "  cuda_source = f.read()\n",
    "print(cuda_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA's constant memory can be used by declaring variables with the `__constant__` memory space specifier.\n",
    "Here, the constant for multiplication is defined in constant memory, and the array length is defined as a regular constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_source = cuda_source.replace('BASIC_ARRAY_OPTRATIONS_3_LENGTH', str(length))\n",
    "module = cp.RawModule(code=cuda_source)\n",
    "module.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compilation, the constant memory pointer is obtained using the `get_global()` function, and the value is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ptr = module.get_global('g_a')\n",
    "a_gpu = cp.ndarray((1), dtype=cp.float32, memptr=ptr)\n",
    "a_gpu[:] = a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPU function is then executed, with an assertion to verify calculation success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cp.array(x, dtype=cp.float32)\n",
    "\n",
    "gpu_func = module.get_function('multConstantMemory')\n",
    "sz_block = 1024,\n",
    "sz_grid = math.ceil(length / sz_block[0]),\n",
    "gpu_func(\n",
    "  block=sz_block, grid=sz_grid,\n",
    "  args=(x_gpu)\n",
    ")\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "\n",
    "x2 = x_gpu.get()\n",
    "err = np.abs((x2 / a) - x)\n",
    "assert np.max(err) < err_eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this notebook explains how to measure execution time using CuPy's `Event` objects.\n",
    "Event objects allow you to record timestamps at specific points on the GPU.\n",
    "The `get_elapsed_time()` function is then used to calculate the elapsed time from these timestamps.\n",
    "\n",
    "This involves creating start and end event objects.\n",
    "`start.record()` marks the beginning of the timed section on the GPU.\n",
    "Immediately after, `start.synchronize()` ensures the CPU waits for this marker to be recorded, guaranteeing accurate timing.\n",
    "The GPU function (your target computation) is then called.\n",
    "Crucially, `cp.cuda.runtime.deviceSynchronize()` is invoked to ensure the GPU device finishes all its pending operations before `end.record()` is called.\n",
    "This is vital because GPU operations are asynchronous.\n",
    "Finally, `end.record()` marks the completion of the timed section, and `end.synchronize()` ensures the CPU waits for this final marker.\n",
    "The elapsed time in milliseconds is then retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 0.517952024936676 [msec]\n"
     ]
    }
   ],
   "source": [
    "start = cp.cuda.Event()\n",
    "end = cp.cuda.Event()\n",
    "\n",
    "start.record()\n",
    "start.synchronize()\n",
    "gpu_func(\n",
    "  block=sz_block, grid=sz_grid,\n",
    "  args=(x_gpu, a, length)\n",
    ")\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "end.record()\n",
    "end.synchronize()\n",
    "msec = cp.cuda.get_elapsed_time(start, end)\n",
    "print('Elapsed Time: {} [msec]'.format(msec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy-cuda-20250531",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
