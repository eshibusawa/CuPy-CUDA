{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f03f8f",
   "metadata": {},
   "source": [
    "# Stencil Patterns with Shared Memory\n",
    "\n",
    "This notebook focuses on implementing stencil patterns, a crucial technique for parallel processing on GPUs.\n",
    "We'll explain these concepts through concrete code examples, combining CuPy's RawModule with CUDA kernels.\n",
    "\n",
    "In this notebook, readers will learn about the fundamental ideas behind stencil patterns and their efficient GPU implementation in a step-by-step manner.\n",
    "To illustrate this, we've chosen the simplest example: a one-dimensional array's moving average.\n",
    "We'll specifically explain Shared Memory, which significantly impacts GPU computation performance.\n",
    "\n",
    "First, we'll demonstrate an implementation where threads within a block use shared memory to share data and collaborate on computations.\n",
    "Subsequently, we'll show an implementation that achieves cooperative processing without Shared Memory, by making the Cooperative Thread Array (CTA) subdivision unit a Warp.\n",
    "This involves data exchange using shuffle instructions. The latter approach allows inter-thread communication without the constraints of Shared Memory.\n",
    "Through these implementations, the concepts of cooperation at both the Warp and block levels will be understood.\n",
    "This will lay the groundwork for utilizing NVIDIA's CUDA universal parallel primitives library, CUB, which we'll cover next time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a42fbe",
   "metadata": {},
   "source": [
    "First, we define `block_size`, which specifies the number of threads in a GPU thread block, and `length`, the total number of elements in the array to be processed.\n",
    "\n",
    "`block_size` determines how many threads constitute a GPU thread block.\n",
    "Threads launched with this `block_size` will cooperate using Shared Memory on the same Streaming Multiprocessor and synchronize with `__syncthreads()`.\n",
    "Here, we've set it to the maximum value of `1024`.\n",
    "`length` specifies the total number of elements in the data array to be processed.\n",
    "In this example, since `length` is greater than `block_size`, a single thread block cannot process all data.\n",
    "When launching multiple thread blocks (`2` blocks in this case) to process data in a distributed manner, kernel code needs to account for \"block boundaries.\"\n",
    "Let's examine how this is handled in the following CUDA kernel code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca8068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extern \"C\" __global__ void movingAverage2(float *y,\n",
      "  const float *__restrict__ x)\n",
      "{\n",
      "  const int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "  if (index >= STENCIL_PATTERNS_1_LENGTH)\n",
      "  {\n",
      "    return;\n",
      "  }\n",
      "\n",
      "  __shared__ float xShared[STENCIL_PATTERNS_1_BLOCK];\n",
      "  xShared[threadIdx.x] = x[index];\n",
      "  __syncthreads();\n",
      "\n",
      "  if (threadIdx.x == (STENCIL_PATTERNS_1_BLOCK - 1))\n",
      "  {\n",
      "    y[index] = (xShared[threadIdx.x] + x[(index + 1) % STENCIL_PATTERNS_1_LENGTH])/2;\n",
      "  }\n",
      "  else\n",
      "  {\n",
      "    y[index] = (xShared[threadIdx.x] + xShared[threadIdx.x + 1])/2;\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "err_eps = 1E-7\n",
    "block_size = 1024\n",
    "length = 2048\n",
    "\n",
    "dn = os.path.join(os.getcwd(), 'kernels')\n",
    "fpfn = os.path.join(dn, '03_stencil_patterns_1.cu')\n",
    "with open(fpfn, 'r') as f:\n",
    "  cuda_source = f.read()\n",
    "print(cuda_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c768bd3",
   "metadata": {},
   "source": [
    "The `movingAverage2` CUDA kernel implements a stencil pattern for calculating the moving average of a one-dimensional array using Shared Memory.\n",
    "\n",
    "First, `xShared`, declared with the `__shared__` memory space specifier, becomes Shared Memory.\n",
    "Each thread copies data from global memory to Shared Memory using `xShared[threadIdx.x] = x[index];`.\n",
    "It's crucial to note that while `index` is the absolute global memory index, `threadIdx.x` is the relative index within the block for Shared Memory.\n",
    "Since Shared Memory is shared within a block, global memory indices should not be used directly.\n",
    "`__syncthreads();` ensures that all threads within the block have completed writing to Shared Memory.\n",
    "\n",
    "Next, the moving average is calculated by reading adjacent elements from Shared Memory.\n",
    "When reading these adjacent elements, two types of boundary handling must be considered: array length boundaries and block boundaries.\n",
    "Particular care is needed for block boundaries; only the thread with the largest `threadIdx.x` directly reads data from global memory again, after considering array length boundaries.\n",
    "\n",
    "Shared Memory in CUDA is a very fast on-chip memory region on the GPU. It offers significantly higher bandwidth and lower latency compared to global memory and is used for data sharing among threads within the same thread block. To achieve this high speed, Shared Memory is divided into multiple \"banks,\" allowing for high parallelism by accessing multiple banks simultaneously. However, if different threads attempt to access the same bank concurrently, a \"bank conflict\" occurs, serializing access and degrading performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_source = cuda_source.replace('STENCIL_PATTERNS_1_BLOCK', str(block_size))\n",
    "cuda_source = cuda_source.replace('STENCIL_PATTERNS_1_LENGTH', str(length))\n",
    "module = cp.RawModule(code=cuda_source)\n",
    "module.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35837966",
   "metadata": {},
   "source": [
    "After embedding constants via string replacement, we compile the `RawModule` and upload arrays to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5570840",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, length, dtype=np.float32)\n",
    "x_gpu = cp.array(x, dtype=cp.float32)\n",
    "y_gpu = cp.empty_like(x)\n",
    "assert x_gpu.flags.c_contiguous\n",
    "assert y_gpu.flags.c_contiguous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfbcb13",
   "metadata": {},
   "source": [
    "We retrieve the CUDA kernel, specify the block and grid sizes, and launch the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355419c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpu_func = module.get_function('movingAverage2')\n",
    "sz_block = block_size,\n",
    "sz_grid = math.ceil(length / sz_block[0]),\n",
    "gpu_func(\n",
    "  block=sz_block, grid=sz_grid,\n",
    "  args=(y_gpu, x_gpu)\n",
    ")\n",
    "cp.cuda.runtime.deviceSynchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea56355",
   "metadata": {},
   "source": [
    "We calculate the correct result using `numpy.roll` for cyclic boundary handling and compare the results.\n",
    "If the computation is successful, no assertion error will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_gpu.get()\n",
    "y_ref = (x + np.roll(x, -1))/2\n",
    "\n",
    "err = np.abs(y_ref - y)\n",
    "assert np.max(err) < err_eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091f080",
   "metadata": {},
   "source": [
    "Next, we'll implement a stencil pattern for calculating the moving average of a one-dimensional array using Warp-level cooperative processing.\n",
    "This approach focuses on thread collaboration within a Warp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b6983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <cooperative_groups.h>\n",
      "namespace cg = cooperative_groups;\n",
      "\n",
      "extern \"C\" __global__ void movingAverage2CTAWarp(float *y,\n",
      "  const float *__restrict__ x)\n",
      "{\n",
      "\tauto cta = cg::this_thread_block();\n",
      "\tconst int ctaIndex = cta.thread_rank();\n",
      "  if (ctaIndex >= STENCIL_PATTERNS_2_LENGTH)\n",
      "  {\n",
      "    return;\n",
      "  }\n",
      "  auto tile = cg::tiled_partition<STENCIL_PATTERNS_2_WARP_SIZE>(cta);\n",
      "  float val = x[ctaIndex];\n",
      "  float average = (tile.shfl_down(val, 1) + val) / 2;\n",
      "  tile.sync();\n",
      "  if (tile.thread_rank() == STENCIL_PATTERNS_2_WARP_SIZE - 1)\n",
      "  {\n",
      "    average = (val + x[(ctaIndex + 1) % STENCIL_PATTERNS_2_LENGTH])/2;\n",
      "  }\n",
      "  cta.sync();\n",
      "  y[ctaIndex] = average;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpfn = os.path.join(dn, '03_stencil_patterns_2.cu')\n",
    "with open(fpfn, 'r') as f:\n",
    "  cuda_source = f.read()\n",
    "print(cuda_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455d6a4",
   "metadata": {},
   "source": [
    "The `movingAverage2CTAWarp` kernel leverages Cooperative Groups library, a powerful CUDA feature.\n",
    "In CUDA, a Warp is the fundamental unit by which the GPU executes and manages threads, typically consisting of 32 threads.\n",
    "These threads operate under a SIMT (Single Instruction, Multiple Threads) architecture, executing the same instruction simultaneously.\n",
    "Cooperative Groups, with `#include <cooperative_groups.h>`, enables flexible management of thread groups and fast data exchange (e.g., shuffle instructions) within a Warp.\n",
    "\n",
    "To compile this kernel using CuPy's default NVRTC backend, just set the `enable_cooperative_groups=True` option when creating `cupy.RawModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d479c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "warp_size = 32\n",
    "cuda_source = cuda_source.replace('STENCIL_PATTERNS_2_LENGTH', str(length))\n",
    "cuda_source = cuda_source.replace('STENCIL_PATTERNS_2_WARP_SIZE', str(warp_size))\n",
    "module = cp.RawModule(code=cuda_source, enable_cooperative_groups=True)\n",
    "module.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d26c75d",
   "metadata": {},
   "source": [
    "We retrieve the newly compiled CUDA kernel from the `RawModule`, specify the block and grid sizes, and launch the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fca429",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpu_func = module.get_function('movingAverage2CTAWarp')\n",
    "sz_block = block_size,\n",
    "sz_grid = math.ceil(length / sz_block[0]),\n",
    "gpu_func(\n",
    "  block=sz_block, grid=sz_grid,\n",
    "  args=(y_gpu, x_gpu)\n",
    ")\n",
    "cp.cuda.runtime.deviceSynchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe94e4",
   "metadata": {},
   "source": [
    "If the computation is successful, no assertion error will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_gpu.get()\n",
    "y_ref = (x + np.roll(x, -1))/2\n",
    "\n",
    "err = np.abs(y_ref - y)\n",
    "assert np.max(err) < err_eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a81e3c",
   "metadata": {},
   "source": [
    "In this notebook, we explained two main implementation methods for stencil patterns in CUDA, using the moving average as an example.\n",
    "\n",
    "First, block-level cooperation using Shared Memory required explicit memory loads for data reuse and synchronization via `__syncthreads()`.\n",
    "In contrast, Warp-level cooperation allowed for faster communication by directly exchanging data between threads within a Warp using shuffle instructions (like `__shfl_sync`). This approach bypasses Shared Memory for inter-thread communication.\n",
    "\n",
    "A key takeaway from both methods was the need for specific considerations for their respective boundary handling (end of block and end of Warp). Additionally, while the Cooperative Groups features used for Warp-level cooperation can be compiled with CuPy's default NVRTC backend by setting `enable_cooperative_groups=True`, using the NVCC backend will become necessary in the next notebook when we integrate the CUB library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy-cuda-20250531",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
